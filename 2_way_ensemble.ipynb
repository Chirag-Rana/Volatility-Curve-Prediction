{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":104024,"databundleVersionId":12520411,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load data\ndata_path = '/kaggle/input/nk-iv-prediction'\ntest = pd.read_parquet(f'{data_path}/test_data.parquet')\nsample_submission = pd.read_csv(f'{data_path}/sample_submission.csv')\nprint(f\"✓ Data loaded successfully - Test: {test.shape}, Sample: {sample_submission.shape}\")\n\ntarget_columns = [col for col in sample_submission.columns if col != 'timestamp']\nsubmission = sample_submission.copy()\nfor col in target_columns:\n    submission[col] = test[col] if col in test.columns else np.nan\nprint(f\"✓ Submission prepared with {len(target_columns)} target columns\")\n\nif 'underlying' in test.columns:\n    submission['underlying'] = test['underlying']\n    all_imputation_cols = target_columns + ['underlying']\n    print(\"✓ Added underlying column as predictor\")\nelse:\n    all_imputation_cols = target_columns\n\n# Model 1: HistGradientBoosting\nprint(\"Setting up Model 1: Enhanced HistGradientBoosting...\")\nimputer1 = IterativeImputer(\n    estimator=HistGradientBoostingRegressor(\n        max_iter=450,            \n        max_depth=16,            \n        learning_rate=0.08,      \n        early_stopping=True,\n        l2_regularization=0.03,  \n        random_state=42\n    ),\n    max_iter=65,                 \n    tol=1e-7,                    \n    n_nearest_features=25,       \n    initial_strategy='median',\n    imputation_order='ascending',\n    random_state=42,\n    verbose=2                    \n)\nprint(\"✓ Model 1 configured: HistGradientBoosting with 450 iter, depth 16\")\n\n# Model 2: BayesianRidge\nprint(\"Setting up Model 2: Enhanced BayesianRidge...\")\nimputer2 = IterativeImputer(\n    estimator=BayesianRidge(\n        alpha_1=5e-7, alpha_2=5e-7,  \n        lambda_1=5e-7, lambda_2=5e-7,\n        compute_score=True\n    ),\n    max_iter=65,\n    tol=1e-7,\n    n_nearest_features=25,\n    initial_strategy='median',\n    imputation_order='ascending',\n    random_state=43,\n    verbose=2               \n)\nprint(\"✓ Model 2 configured: BayesianRidge with enhanced precision\")\n\nfit_subset_size = 8500\nfit_subset = submission[all_imputation_cols].iloc[:fit_subset_size].copy()\n\nimputer1.fit(fit_subset)\nimputer2.fit(fit_subset)\n\n# Generate predictions\nimputed1 = imputer1.transform(submission[all_imputation_cols])\nimputed2 = imputer2.transform(submission[all_imputation_cols])\n\nif 'underlying' in all_imputation_cols:\n    pred1 = imputed1[:, :-1]\n    pred2 = imputed2[:, :-1]\nelse:\n    pred1 = imputed1\n    pred2 = imputed2\n\nprint(f\"✓ Prediction arrays ready: {pred1.shape}\")\n\n# Test weights\nweight_combinations = [\n    (0.67, 0.33),  \n    (0.69, 0.31),  \n    (0.65, 0.35),  \n    (0.71, 0.29),  \n]\n\nfor i, (w1, w2) in enumerate(weight_combinations, 1):\n    \n    test_submission = submission.copy()\n    ensemble_result = w1 * pred1 + w2 * pred2\n    test_submission[target_columns] = ensemble_result\n    \n    # Preserve existing values\n    values_preserved = 0\n    for col in target_columns:\n        if col in test.columns:\n            existing_mask = ~test[col].isna()\n            if existing_mask.sum() > 0:\n                test_submission.loc[existing_mask, col] = test.loc[existing_mask, col]\n                values_preserved += existing_mask.sum()\n    \n    # Conservative post-processing\n    test_submission[target_columns] = test_submission[target_columns].clip(0.015, 2.8)\n    test_submission = test_submission.drop_duplicates(subset=['timestamp'])\n    \n    # Calculate MSE\n    overall_errors = []\n    for col in target_columns:\n        if col in test.columns:\n            mask = ~test[col].isna()\n            if mask.sum() > 0:\n                errors = (test.loc[mask, col] - test_submission.loc[mask, col])**2\n                overall_errors.extend(errors)\n    \n    if overall_errors:\n        current_mse = np.mean(overall_errors)\n        \n        if current_mse < best_mse:\n            best_mse = current_mse\n            best_result = test_submission[['timestamp'] + target_columns].copy()\n            best_weights = (w1, w2)\n        else:\n            difference = ((current_mse - best_mse) / best_mse) * 100\n    else:\n        print(\"❌ No valid MSE calculation possible\")\n\n# Save best result\nif best_result is not None:\n    best_result.to_csv('submission.csv', index=False)\n    print(\"submission saved!\")\nelse:\n    print(\"❌ ERROR: No valid results generated\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T16:44:01.538209Z","iopub.execute_input":"2025-06-15T16:44:01.538538Z","iopub.status.idle":"2025-06-15T16:45:00.055273Z","shell.execute_reply.started":"2025-06-15T16:44:01.538509Z","shell.execute_reply":"2025-06-15T16:45:00.049935Z"}},"outputs":[],"execution_count":null}]}