{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":104024,"databundleVersionId":12520411,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"=== 4-WAY ENSEMBLE WITH 10,000 WEIGHT COMBOS ===\")\n\n# Load data\ndata_path = '/kaggle/input/nk-iv-prediction'\ntest = pd.read_parquet(f'{data_path}/test_data.parquet')\nsample_submission = pd.read_csv(f'{data_path}/sample_submission.csv')\n\n# Load provided file for comparison\nprovided_file_path = '/kaggle/input/prev/prev_best_submission.csv'\nprovided_data = pd.read_csv(provided_file_path)\nprint(f\"✓ Loaded provided file: {provided_data.shape}\")\n\n# Prepare submission\ntarget_columns = [col for col in sample_submission.columns if col != 'timestamp']\nsubmission = sample_submission.copy()\nfor col in target_columns:\n    submission[col] = test[col] if col in test.columns else np.nan\n\n# Add underlying for imputation context\nif 'underlying' in test.columns:\n    submission['underlying'] = test['underlying']\n    all_imputation_cols = target_columns + ['underlying']\n    print(\"✓ Added underlying for imputation context\")\nelse:\n    all_imputation_cols = target_columns\n\nprint(f\"✓ Submission prepared: {submission.shape}\")\n\n# 1. HistGradientBoosting\nimputer_hist = IterativeImputer(\n    estimator=HistGradientBoostingRegressor(\n        max_iter=400,\n        max_depth=15,\n        learning_rate=0.1,\n        early_stopping=True,\n        random_state=42\n    ),\n    max_iter=60,\n    tol=1e-6,\n    n_nearest_features=30,\n    initial_strategy='median',\n    imputation_order='ascending',\n    random_state=42,\n    verbose=2\n)\n\n# 2. BayesianRidge\nimputer_bayes = IterativeImputer(\n    estimator=BayesianRidge(\n        alpha_1=1,\n        alpha_2=1,\n        lambda_1=1,\n        lambda_2=1,\n        compute_score=True\n    ),\n    max_iter=60,\n    tol=1e-6,\n    n_nearest_features=30,\n    initial_strategy='median',\n    imputation_order='ascending',\n    random_state=43,\n    verbose=2\n)\n\n# 3. XGBoost\nimputer_xgb = IterativeImputer(\n    estimator=XGBRegressor(\n        n_estimators=400,        \n        max_depth=6,             \n        learning_rate=0.05,      \n        subsample=0.9,           \n        colsample_bytree=0.8,    \n        tree_method='hist',      \n        random_state=42,\n        verbosity=0\n    ),\n    max_iter=60,                 \n    tol=1e-6,                    \n    n_nearest_features=30,       \n    initial_strategy='median',   \n    imputation_order='ascending', \n    random_state=44,\n    verbose=2\n)\n\n# 4. LightGBM\nimputer_lgb = IterativeImputer(\n    estimator=lgb.LGBMRegressor(\n        n_estimators=1000,\n        max_depth=6,\n        learning_rate=0.1,\n        num_leaves=63,\n        min_data_in_leaf=50,\n        lambda_l1=0.0,\n        lambda_l2=0.1,\n        max_bin=255,\n        bagging_fraction=0.85,\n        feature_fraction=0.85,\n        verbose=-1,\n        random_state=42\n    ),\n    max_iter=60,\n    tol=1e-6,\n    n_nearest_features=30,\n    initial_strategy='median',\n    imputation_order='ascending',\n    random_state=45,\n    verbose=2\n)\n\n# TRAIN ALL 4 IMPUTERS\nfull_training_data = submission[all_imputation_cols].copy()\n\nimputer_hist.fit(full_training_data)\nimputer_bayes.fit(full_training_data)\nimputer_xgb.fit(full_training_data)\nimputer_lgb.fit(full_training_data)\n\n# GENERATE PREDICTIONS FROM ALL 4 IMPUTERS\n\nimputed_hist = imputer_hist.transform(submission[all_imputation_cols])\nprint(\"✓ HistGradientBoosting predictions generated\")\n\nprint(\"Generating BayesianRidge predictions...\")\nimputed_bayes = imputer_bayes.transform(submission[all_imputation_cols])\nprint(\"✓ BayesianRidge predictions generated\")\n\nprint(\"Generating XGBoost predictions...\")\nimputed_xgb = imputer_xgb.transform(submission[all_imputation_cols])\nprint(\"✓ XGBoost predictions generated\")\n\nprint(\"Generating LightGBM predictions...\")\nimputed_lgb = imputer_lgb.transform(submission[all_imputation_cols])\nprint(\"✓ LightGBM predictions generated\")\n\n# Extract only target columns (exclude underlying)\nif 'underlying' in all_imputation_cols:\n    pred_hist = imputed_hist[:, :-1]\n    pred_bayes = imputed_bayes[:, :-1]\n    pred_xgb = imputed_xgb[:, :-1]\n    pred_lgb = imputed_lgb[:, :-1]\nelse:\n    pred_hist = imputed_hist\n    pred_bayes = imputed_bayes\n    pred_xgb = imputed_xgb\n    pred_lgb = imputed_lgb\n\nprint(\"✓ All predictions extracted (target columns only)\")\n\n# GENERATE 10,000 RANDOM WEIGHT COMBINATIONS\nprint(\"\\n=== GENERATING 10,000 RANDOM WEIGHT COMBINATIONS ===\")\nnp.random.seed(42)\nweights = np.random.dirichlet(np.ones(4), size=10000)\nprint(\"✓ 10,000 weight combinations generated using Dirichlet distribution\")\n\n# TEST ALL 10,000 WEIGHT COMBINATIONS\nprint(f\"\\n=== TESTING ALL 10,000 WEIGHT COMBINATIONS ===\")\nprint(\"Format: HistGrad, Bayes, XGBoost, LightGBM\")\n\nbest_mse = float('inf')\nbest_result = None\nbest_weights = None\nbest_combo_id = None\n\nfor i, w in enumerate(weights, 1):\n    weight_hist, weight_bayes, weight_xgb, weight_lgb = w\n    \n    # Create 4-way ensemble\n    ensemble_pred = (weight_hist * pred_hist + \n                    weight_bayes * pred_bayes + \n                    weight_xgb * pred_xgb + \n                    weight_lgb * pred_lgb)\n    \n    # Create test submission\n    test_submission = submission[['timestamp'] + target_columns].copy()\n    test_submission[target_columns] = ensemble_pred\n    \n    # Preserve existing values exactly\n    values_preserved = 0\n    for col in target_columns:\n        if col in test.columns:\n            existing_mask = ~test[col].isna()\n            if existing_mask.sum() > 0:\n                test_submission.loc[existing_mask, col] = test.loc[existing_mask, col]\n                values_preserved += existing_mask.sum()\n    \n    # Post-processing\n    test_submission[target_columns] = test_submission[target_columns].clip(0.01, 3.0)\n    test_submission = test_submission.drop_duplicates(subset=['timestamp'])\n    \n    # Calculate MSE vs provided file\n    provided_errors = []\n    comparisons_made = 0\n    \n    for col in target_columns:\n        if col in provided_data.columns:\n            merged = pd.merge(test_submission[['timestamp', col]], \n                            provided_data[['timestamp', col]], \n                            on='timestamp', suffixes=('_pred', '_true'))\n            \n            valid_mask = ~merged[col + '_true'].isna() & ~merged[col + '_pred'].isna()\n            if valid_mask.sum() > 0:\n                errors = (merged.loc[valid_mask, col + '_true'] - merged.loc[valid_mask, col + '_pred'])**2\n                provided_errors.extend(errors)\n                comparisons_made += valid_mask.sum()\n    \n    current_mse = np.mean(provided_errors) if provided_errors else float('inf')\n    \n    # Track best result\n    if current_mse < best_mse:\n        best_mse = current_mse\n        best_result = test_submission.copy()\n        best_weights = (weight_hist, weight_bayes, weight_xgb, weight_lgb)\n        best_combo_id = i\n\n# SAVE BEST RESULT\nif best_result is not None:\n    best_result.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}